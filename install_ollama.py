#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de instalaci√≥n y configuraci√≥n de Ollama con Qwen y Llama
"""

import os
import sys
import subprocess
import json
import logging
import requests
import time
from pathlib import Path
from typing import List, Dict, Optional

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OllamaInstaller:
    """
    Instalador y configurador de Ollama con modelos Qwen y Llama
    """
    
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.models_to_install = [
            # Modelos principales (alta prioridad)
            "deepseek-r1:7b",      # Razonamiento profundo y an√°lisis avanzado
            "gemma2:2b",           # Procesamiento r√°pido y eficiente
            "qwen:7b",             # An√°lisis multiidioma y general
            
            # Modelos secundarios (media prioridad)
            "llama3.1:8b",         # Generaci√≥n de texto
            "gemma2:9b",           # An√°lisis avanzado
            "qwen2.5-coder:7b",    # Tareas t√©cnicas y programaci√≥n
            
            # Modelos avanzados (baja prioridad - solo si hay recursos)
            "deepseek-r1:14b"      # Razonamiento complejo (requiere m√°s RAM)
        ]
        
    def check_ollama_installation(self) -> bool:
        """
        Verificar si Ollama est√° instalado
        """
        try:
            result = subprocess.run(
                ["ollama", "version"], 
                capture_output=True, 
                text=True,
                timeout=10
            )
            if result.returncode == 0:
                logger.info(f"‚úÖ Ollama instalado: {result.stdout.strip()}")
                return True
            return False
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False
            
    def install_ollama_windows(self) -> bool:
        """
        Instalar Ollama en Windows
        """
        logger.info("üöÄ Instalando Ollama para Windows...")
        
        try:
            # URL de descarga para Windows
            download_url = "https://ollama.com/download/windows"
            
            logger.info("üì• Descargando Ollama...")
            logger.info(f"Por favor, visita: {download_url}")
            logger.info("Y sigue las instrucciones de instalaci√≥n.")
            
            # Esperar confirmaci√≥n del usuario
            input("\nPresiona Enter cuando hayas completado la instalaci√≥n de Ollama...")
            
            # Verificar instalaci√≥n
            if self.check_ollama_installation():
                logger.info("‚úÖ Ollama instalado correctamente")
                return True
            else:
                logger.error("‚ùå Ollama no se detecta despu√©s de la instalaci√≥n")
                return False
                
        except Exception as e:
            logger.error(f"Error instalando Ollama: {e}")
            return False
            
    def start_ollama_service(self) -> bool:
        """
        Iniciar el servicio de Ollama
        """
        try:
            logger.info("üöÄ Iniciando servicio Ollama...")
            
            # En Windows, Ollama se ejecuta como servicio
            # Intentar iniciar el servicio
            process = subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                creationflags=subprocess.CREATE_NEW_CONSOLE if os.name == 'nt' else 0
            )
            
            # Esperar un poco para que el servicio inicie
            time.sleep(5)
            
            # Verificar si el servicio est√° corriendo
            if self.check_ollama_running():
                logger.info("‚úÖ Servicio Ollama iniciado correctamente")
                return True
            else:
                logger.warning("‚ö†Ô∏è El servicio puede estar iniciando, continuando...")
                return True
                
        except Exception as e:
            logger.error(f"Error iniciando servicio Ollama: {e}")
            return False
            
    def check_ollama_running(self) -> bool:
        """
        Verificar si Ollama est√° corriendo
        """
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
            
    def wait_for_ollama(self, timeout: int = 60) -> bool:
        """
        Esperar a que Ollama est√© disponible
        """
        logger.info("‚è≥ Esperando a que Ollama est√© disponible...")
        
        for i in range(timeout):
            if self.check_ollama_running():
                logger.info("‚úÖ Ollama est√° disponible")
                return True
            time.sleep(1)
            if i % 10 == 0:
                logger.info(f"‚è≥ Esperando... ({i}/{timeout}s)")
                
        logger.error("‚ùå Timeout esperando a Ollama")
        return False
        
    def install_model(self, model_name: str) -> bool:
        """
        Instalar un modelo espec√≠fico
        """
        try:
            logger.info(f"üì¶ Instalando modelo {model_name}...")
            
            # Usar el comando pull de Ollama
            process = subprocess.Popen(
                ["ollama", "pull", model_name],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                universal_newlines=True
            )
            
            # Mostrar progreso en tiempo real
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    # Filtrar l√≠neas de progreso
                    if "pulling" in output.lower() or "downloading" in output.lower():
                        logger.info(f"üì• {output.strip()}")
                    elif "success" in output.lower():
                        logger.info(f"‚úÖ {output.strip()}")
                        
            rc = process.poll()
            if rc == 0:
                logger.info(f"‚úÖ Modelo {model_name} instalado correctamente")
                return True
            else:
                logger.error(f"‚ùå Error instalando {model_name}")
                return False
                
        except Exception as e:
            logger.error(f"Error instalando modelo {model_name}: {e}")
            return False
            
    def get_installed_models(self) -> List[str]:
        """
        Obtener lista de modelos instalados
        """
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                models = data.get('models', [])
                return [model.get('name', '') for model in models]
            return []
        except Exception as e:
            logger.error(f"Error obteniendo modelos: {e}")
            return []
            
    def test_model(self, model_name: str) -> bool:
        """
        Probar un modelo instalado
        """
        try:
            logger.info(f"üß™ Probando modelo {model_name}...")
            
            test_payload = {
                "model": model_name,
                "prompt": "Hola, explica qu√© eres en una l√≠nea.",
                "stream": False
            }
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=test_payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                output = result.get('response', '').strip()
                logger.info(f"‚úÖ {model_name} responde: {output[:100]}...")
                return True
            else:
                logger.error(f"‚ùå Error probando {model_name}: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"Error probando modelo {model_name}: {e}")
            return False
            
    def setup_environment_variables(self):
        """
        Configurar variables de entorno
        """
        env_file = Path(".env")
        
        env_vars = {
            "OLLAMA_BASE_URL": "http://localhost:11434",
            "OLLAMA_TIMEOUT": "300",
            "AI_PROVIDER_PRIORITY": "ollama,groq",
            "USE_LOCAL_AI": "true"
        }
        
        logger.info("‚öôÔ∏è Configurando variables de entorno...")
        
        # Leer archivo .env existente
        existing_vars = {}
        if env_file.exists():
            with open(env_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if '=' in line and not line.startswith('#'):
                        key, value = line.split('=', 1)
                        existing_vars[key] = value
                        
        # Actualizar con nuevas variables
        existing_vars.update(env_vars)
        
        # Escribir archivo .env actualizado
        with open(env_file, 'w', encoding='utf-8') as f:
            f.write("# Configuraci√≥n de Ollama\n")
            for key, value in env_vars.items():
                f.write(f"{key}={value}\n")
            f.write("\n")
            
            # Escribir otras variables existentes
            for key, value in existing_vars.items():
                if key not in env_vars:
                    f.write(f"{key}={value}\n")
                    
        logger.info("‚úÖ Variables de entorno configuradas")
        
    def create_installation_summary(self, installed_models: List[str]):
        """
        Crear resumen de instalaci√≥n
        """
        summary = f"""
üéâ INSTALACI√ìN DE OLLAMA COMPLETADA

üìã Resumen:
- Ollama: ‚úÖ Instalado y corriendo
- URL: {self.ollama_url}
- Modelos instalados: {len(installed_models)}

ü§ñ Modelos disponibles:
"""
        for model in installed_models:
            summary += f"  - {model}\n"
            
        summary += """
üöÄ Pr√≥ximos pasos:
1. El sistema ahora puede usar IA local con Ollama
2. Reinicia la aplicaci√≥n para aplicar los cambios
3. Los an√°lisis usar√°n modelos locales por defecto

üí° Comandos √∫tiles:
- Ver modelos: ollama list
- Ejecutar modelo: ollama run <modelo>
- Detener servicio: ollama stop (si es necesario)

‚úÖ ¬°Ollama est√° listo para usar!
"""
        
        logger.info(summary)
        
        # Guardar resumen en archivo
        with open("ollama_installation_summary.txt", "w", encoding="utf-8") as f:
            f.write(summary)
            
    def run_installation(self) -> bool:
        """
        Ejecutar proceso completo de instalaci√≥n
        """
        logger.info("üöÄ Iniciando instalaci√≥n de Ollama con Qwen y Llama")
        
        # 1. Verificar si Ollama ya est√° instalado
        if not self.check_ollama_installation():
            logger.info("üì¶ Ollama no est√° instalado")
            if os.name == 'nt':  # Windows
                if not self.install_ollama_windows():
                    return False
            else:
                logger.error("‚ùå Instalaci√≥n autom√°tica solo soportada en Windows")
                logger.info("Por favor, instala Ollama manualmente desde: https://ollama.com")
                return False
                
        # 2. Iniciar servicio si no est√° corriendo
        if not self.check_ollama_running():
            if not self.start_ollama_service():
                logger.error("‚ùå No se pudo iniciar el servicio Ollama")
                return False
                
        # 3. Esperar a que Ollama est√© disponible
        if not self.wait_for_ollama():
            return False
            
        # 4. Instalar modelos requeridos
        installed_models = self.get_installed_models()
        logger.info(f"üìã Modelos ya instalados: {installed_models}")
        
        success_count = 0
        for model in self.models_to_install:
            if model not in installed_models:
                if self.install_model(model):
                    success_count += 1
                    # Probar el modelo
                    self.test_model(model)
                else:
                    logger.warning(f"‚ö†Ô∏è No se pudo instalar {model}")
            else:
                logger.info(f"‚úÖ Modelo {model} ya est√° instalado")
                success_count += 1
                
        # 5. Configurar variables de entorno
        self.setup_environment_variables()
        
        # 6. Verificaci√≥n final
        final_models = self.get_installed_models()
        logger.info(f"üìã Modelos finales instalados: {final_models}")
        
        # 7. Crear resumen
        self.create_installation_summary(final_models)
        
        if success_count >= 2:  # Al menos 2 modelos instalados
            logger.info("üéâ Instalaci√≥n completada exitosamente")
            return True
        else:
            logger.error("‚ùå La instalaci√≥n no se complet√≥ correctamente")
            return False

def main():
    """
    Funci√≥n principal
    """
    installer = OllamaInstaller()
    
    try:
        success = installer.run_installation()
        if success:
            print("\n" + "="*60)
            print("üéâ OLLAMA INSTALADO EXITOSAMENTE")
            print("="*60)
            print("El sistema ahora puede usar IA local.")
            print("Reinicia la aplicaci√≥n para aplicar los cambios.")
            print("="*60)
        else:
            print("\n" + "="*60)
            print("‚ùå ERROR EN LA INSTALACI√ìN")
            print("="*60)
            print("Revisa los logs para m√°s detalles.")
            print("="*60)
            
    except KeyboardInterrupt:
        print("\n‚ùå Instalaci√≥n cancelada por el usuario")
    except Exception as e:
        logger.error(f"Error inesperado: {e}")
        print(f"\n‚ùå Error inesperado: {e}")

if __name__ == "__main__":
    main()
