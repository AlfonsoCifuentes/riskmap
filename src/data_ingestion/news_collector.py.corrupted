"""
News collection module for gathering articles from multiple sources.
Supports multilingual collection (Spanish, English, Russian, Chinese, Arabic)    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """Parse ISO date string to datetime object."""
        if not date_str:
            return None
        
        try:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except ValueError:
            logger.warning(f"Could not parse date: {date_str}")
            return None

    def _save_articles(self, articles: List[Dict[str, Any]]):
        """Save articles to database."""
        conn = self.db.get_connection()
        cursor = conn.cursor()
        
        saved_count = 0
        for article in articles:
            try:
                cursor.execute('''
                    INSERT OR IGNORE INTO articles 
                    (title, content, url, source, published_at, language)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    article['title'],
                    article['content'],
                    article['url'],
                    article['source'],
                    article['published_at'],
                    article['language']
                ))
                
                if cursor.rowcount > 0:
                    saved_count += 1
                    
            except sqlite3.Error as e:
                logger.error(f"Error saving article: {e}")
        
        conn.commit()
        conn.close()
        
        if saved_count > 0:
            logger.info(f"Saved {saved_count} new articles to database")import requests
import feedparser
import sqlite3
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import time
import logging
from pathlib import Path
import sys

# Add src to path
sys.path.append(str(Path(__file__).parent.parent))

from utils.config import config, DatabaseManager

logger = logging.getLogger(__name__)


class NewsAPICollector:
    """Collector for NewsAPI.org service."""
    
    def __init__(self):
        self.api_key = config.get_newsapi_key()
        self.base_url = config.get('data_sources.newsapi.base_url', 'https://newsapi.org/v2')
        self.supported_languages = config.get_supported_languages()
        self.db = DatabaseManager(config)
        
        if not self.api_key:
            logger.warning("NewsAPI key not configured. Some features may not work.")
    
    def collect_headlines(self, language: str = 'en', category: str = None, 
                         country: str = None, max_articles: int = 100) -> List[Dict[str, Any]]:
        """Collect top headlines from NewsAPI."""
        if not self.api_key:
            logger.error("NewsAPI key not available")
            return []
        
        url = f"{self.base_url}/top-headlines"
        params = {
            'apiKey': self.api_key,
            'language': language,
            'pageSize': min(max_articles, 100)
        }
        
        if category:
            params['category'] = category
        if country:
            params['country'] = country
        
        try:
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            articles = data.get('articles', [])
            
            logger.info(f"Collected {len(articles)} headlines for language {language}")
            formatted_articles = self._format_articles(articles, language)
            
            # Save articles automatically
            if formatted_articles:
                self._save_articles(formatted_articles)
            
            return formatted_articles
            
        except requests.RequestException as e:
            logger.error(f"Error collecting headlines: {e}")
            return []
    
    def search_everything(self, query: str, language: str = 'en', 
                         from_date: str = None, max_articles: int = 100) -> List[Dict[str, Any]]:
        """Search everything endpoint for specific queries."""
        if not self.api_key:
            logger.error("NewsAPI key not available")
            return []
        
        url = f"{self.base_url}/everything"
        params = {
            'apiKey': self.api_key,
            'q': query,
            'language': language,
            'pageSize': min(max_articles, 100),
            'sortBy': 'publishedAt'
        }
        
        if from_date:
            params['from'] = from_date
        
        try:
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            articles = data.get('articles', [])
            
            logger.info(f"Found {len(articles)} articles for query '{query}' in {language}")
            formatted_articles = self._format_articles(articles, language)
            
            # Save articles automatically
            if formatted_articles:
                self._save_articles(formatted_articles)
            
            return formatted_articles
            
        except requests.RequestException as e:
            logger.error(f"Error searching articles: {e}")
            return []
    
    def _format_articles(self, articles: List[Dict], language: str) -> List[Dict[str, Any]]:
        """Format articles from NewsAPI response."""
        formatted_articles = []
        
        for article in articles:
            formatted_article = {
                'title': article.get('title', ''),
                'content': article.get('content', '') or article.get('description', '') or '',
                'url': article.get('url', ''),
                'source': article.get('source', {}).get('name', 'Unknown'),
                'published_at': self._parse_date(article.get('publishedAt')),
                'language': language,
                'author': article.get('author', ''),
                'image_url': article.get('urlToImage', '')
            }
            
            # Filter out articles with insufficient content
            content = formatted_article.get('content', '') or ''
            if len(content) > 50:
                formatted_articles.append(formatted_article)
        
        return formatted_articles
    
    def collect_by_keyword(self, keywords: List[str], language: str = 'en', 
                          max_articles: int = 100) -> List[Dict[str, Any]]:
        """Collect articles by keywords using everything endpoint."""
        if not keywords:
            return []
        
        query = ' OR '.join(f'"{keyword}"' for keyword in keywords)
        from_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        
        return self.search_everything(
            query=query,
            language=language,
            from_date=from_date,
            max_articles=max_articles
        )
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """Parse date string from NewsAPI."""
        if not date_str:
            return None
        
        try:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except ValueError:
            logger.warning(f"Could not parse date: {date_str}")
            return None


class RSSCollector:
    """Collector for RSS feeds from various news sources."""
    
    def __init__(self):
        self.sources = config.get('data_sources.manual_sources', [])
    
    def collect_from_feed(self, feed_url: str, language: str) -> List[Dict[str, Any]]:
        """Collect articles from RSS feed."""
        try:
            feed = feedparser.parse(feed_url)
            articles = []
            
            for entry in feed.entries[:50]:  # Limit to 50 articles per feed
                article = {
                    'title': entry.get('title', ''),
                    'content': entry.get('summary', '') or entry.get('description', ''),
                    'url': entry.get('link', ''),
                    'source': feed.feed.get('title', 'RSS Feed'),
                    'published_at': self._parse_rss_date(entry.get('published')),
                    'language': language
                }
                
                if len(article['content']) > 50:
                    articles.append(article)
            
            logger.info(f"Collected {len(articles)} articles from RSS feed: {feed_url}")
            return articles
            
        except Exception as e:
            logger.error(f"Error collecting from RSS feed {feed_url}: {e}")
            return []
    
    def collect_all_feeds(self) -> List[Dict[str, Any]]:
        """Collect from all configured RSS feeds."""
        all_articles = []
        
        for source in self.sources:
            url = source.get('url')
            language = source.get('language', 'en')
            name = source.get('name', 'Unknown')
            
            logger.info(f"Collecting from {name} ({language})")
            articles = self.collect_from_feed(url, language)
            all_articles.extend(articles)
            
            # Be respectful to servers
            time.sleep(1)
        
        return all_articles
    
    def _parse_rss_date(self, date_str: str) -> Optional[datetime]:
        """Parse date string from RSS feed."""
        if not date_str:
            return None
        
        try:
            return datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %z')
        except ValueError:
            try:
                return datetime.strptime(date_str, '%Y-%m-%dT%H:%M:%S%z')
            except ValueError:
                logger.warning(f"Could not parse RSS date: {date_str}")
                return None


class GeopoliticalNewsCollector:
    """Main collector class for geopolitical news from multiple sources."""
    
    def __init__(self):
        self.newsapi = NewsAPICollector()
        self.rss = RSSCollector()
        self.db = DatabaseManager(config)
        self.db.create_tables()
        
        # Geopolitical keywords for targeted collection
        self.geopolitical_keywords = {
            'en': ['conflict', 'war', 'diplomacy', 'sanctions', 'protest', 'crisis', 'military', 'government'],
            'es': ['conflicto', 'guerra', 'diplomacia', 'sanciones', 'protesta', 'crisis', 'militar', 'gobierno'],
            'ru': ['конфликт', 'война', 'дипломатия', 'санкции', 'протест', 'кризис', 'военный', 'правительство'],
            'zh': ['冲突', '战争', '外交', '制裁', '抗议', '危机', '军事', '政府'],
            'ar': ['صراع', 'حرب', 'دبلوماسية', 'عقوبات', 'احتجاج', 'أزمة', 'عسكري', 'حكومة']
        }
    
    def collect_daily_news(self) -> int:
        """Collect daily news from all sources."""
        logger.info("Starting daily news collection")
        total_articles = 0
        
        # Collect from NewsAPI for each supported language
        for language in config.get_supported_languages():
            # Get general headlines
            headlines = self.newsapi.collect_headlines(language=language, max_articles=50)
            total_articles += len(headlines)
            self._save_articles(headlines)
            
            # Search for specific geopolitical terms
            keywords = self.geopolitical_keywords.get(language, [])
            for keyword in keywords[:3]:  # Limit to 3 keywords per language to avoid API limits
                articles = self.newsapi.search_everything(
                    query=keyword, 
                    language=language, 
                    from_date=(datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'),
                    max_articles=20
                )
                total_articles += len(articles)
                self._save_articles(articles)
                
                # Respect API rate limits
                time.sleep(2)
        
        # Collect from RSS feeds
        rss_articles = self.rss.collect_all_feeds()
        total_articles += len(rss_articles)
        self._save_articles(rss_articles)
        
        logger.info(f"Collected {total_articles} articles total")
        return total_articles
    
    def _save_articles(self, articles: List[Dict[str, Any]]):
        """Save articles to database."""
        conn = self.db.get_connection()
        cursor = conn.cursor()
        
        saved_count = 0
        for article in articles:
            try:
                cursor.execute('''
                    INSERT OR IGNORE INTO articles 
                    (title, content, url, source, published_at, language)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    article['title'],
                    article['content'],
                    article['url'],
                    article['source'],
                    article['published_at'],
                    article['language']
                ))
                
                if cursor.rowcount > 0:
                    saved_count += 1
                    
            except sqlite3.Error as e:
                logger.error(f"Error saving article: {e}")
        
        conn.commit()
        conn.close()
        
        if saved_count > 0:
            logger.info(f"Saved {saved_count} new articles to database")
    
    def get_recent_articles(self, days: int = 7, language: str = None) -> List[Dict[str, Any]]:
        """Get recent articles from database."""
        conn = self.db.get_connection()
        cursor = conn.cursor()
        
        query = '''
            SELECT id, title, content, url, source, published_at, language
            FROM articles 
            WHERE published_at > datetime('now', '-{} days')
        '''.format(days)
        
        params = []
        if language:
            query += ' AND language = ?'
            params.append(language)
        
        query += ' ORDER BY published_at DESC'
        
        cursor.execute(query, params)
        articles = cursor.fetchall()
        conn.close()
        
        return [
            {
                'id': row[0],
                'title': row[1],
                'content': row[2],
                'url': row[3],
                'source': row[4],
                'published_at': row[5],
                'language': row[6]
            }
            for row in articles
        ]


def main():
    """Main function for running news collection."""
    collector = GeopoliticalNewsCollector()
    
    try:
        total_articles = collector.collect_daily_news()
        print(f"Successfully collected {total_articles} articles")
        
        # Show recent articles summary
        recent_articles = collector.get_recent_articles(days=1)
        print(f"Total articles in last 24 hours: {len(recent_articles)}")
        
        # Language breakdown
        language_counts = {}
        for article in recent_articles:
            lang = article['language']
            language_counts[lang] = language_counts.get(lang, 0) + 1
        
        print("Language breakdown:")
        for lang, count in language_counts.items():
            print(f"  {lang}: {count} articles")
            
    except Exception as e:
        logger.error(f"Error in main collection process: {e}")
        raise


if __name__ == "__main__":
    main()
