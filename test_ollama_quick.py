#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test rÃ¡pido del sistema de IA unificado con Ollama
"""

import os
import sys
from pathlib import Path

# Agregar el directorio raÃ­z al path
sys.path.append(str(Path(__file__).parent))

def test_ollama_integration():
    """Probar integraciÃ³n con Ollama"""
    print("ğŸ§ª Probando integraciÃ³n con Ollama...")
    
    try:
        from src.ai.ollama_service import ollama_service, OllamaModel
        
        # Verificar estado
        status = ollama_service.check_ollama_status()
        print(f"ğŸ“Š Ollama disponible: {status}")
        
        if not status:
            print("âŒ Ollama no estÃ¡ corriendo. Inicia con: ollama serve")
            return False
            
        # Listar modelos
        models = ollama_service.get_available_models()
        print(f"ğŸ¤– Modelos disponibles: {len(models)}")
        
        specialized_models = {
            'deepseek': False,
            'gemma': False, 
            'qwen': False,
            'llama': False
        }
        
        for model in models:
            name = model.get('name', '').lower()
            print(f"  - {model.get('name', 'Unknown')}")
            
            if 'deepseek' in name:
                specialized_models['deepseek'] = True
            elif 'gemma' in name:
                specialized_models['gemma'] = True
            elif 'qwen' in name:
                specialized_models['qwen'] = True
            elif 'llama' in name:
                specialized_models['llama'] = True
        
        print(f"\nğŸ¯ Modelos especializados disponibles:")
        print(f"  - DeepSeek (anÃ¡lisis profundo): {'âœ…' if specialized_models['deepseek'] else 'âŒ'}")
        print(f"  - Gemma (resÃºmenes rÃ¡pidos): {'âœ…' if specialized_models['gemma'] else 'âŒ'}")
        print(f"  - Qwen (multiidioma): {'âœ…' if specialized_models['qwen'] else 'âŒ'}")
        print(f"  - Llama (propÃ³sito general): {'âœ…' if specialized_models['llama'] else 'âŒ'}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error probando Ollama: {e}")
        return False

def test_unified_service():
    """Probar servicio unificado"""
    print("\nğŸ§ª Probando servicio unificado...")
    
    try:
        from src.ai.unified_ai_service import unified_ai_service
        
        # Obtener estado
        status = unified_ai_service.get_service_status()
        
        print(f"ğŸ“Š Estado general:")
        print(f"  - Ollama: {'âœ…' if status['ollama']['available'] else 'âŒ'}")
        print(f"  - Groq: {'âœ…' if status['groq']['available'] else 'âŒ'}")
        print(f"  - Proveedor preferido: {status['preferred_provider']}")
        
        capabilities = status['capabilities']
        print(f"\nğŸš€ Capacidades avanzadas:")
        print(f"  - Razonamiento profundo: {'âœ…' if capabilities['deep_reasoning'] else 'âŒ'}")
        print(f"  - Procesamiento rÃ¡pido: {'âœ…' if capabilities['fast_processing'] else 'âŒ'}")
        print(f"  - Soporte multiidioma: {'âœ…' if capabilities['multilingual'] else 'âŒ'}")
        print(f"  - IA de propÃ³sito general: {'âœ…' if capabilities['general_purpose'] else 'âŒ'}")
        
        return status['ollama']['available']
        
    except Exception as e:
        print(f"âŒ Error probando servicio unificado: {e}")
        return False

def test_simple_analysis():
    """Probar anÃ¡lisis simple con Ollama"""
    print("\nğŸ§ª Probando anÃ¡lisis simple...")
    
    try:
        from src.ai.ollama_service import ollama_service, OllamaModel
        
        test_content = "Escalada de tensiones diplomÃ¡ticas entre paÃ­ses vecinos por disputas territoriales"
        
        print("ğŸ”„ Ejecutando anÃ¡lisis con DeepSeek...")
        
        # AnÃ¡lisis con DeepSeek
        result = ollama_service.analyze_geopolitical_content(
            content=test_content,
            model=OllamaModel.DEEPSEEK_R1_7B
        )
        
        if result:
            print("âœ… AnÃ¡lisis completado")
            print(f"ğŸ“„ Resumen: {result.get('summary', 'N/A')[:150]}...")
            print(f"ğŸ¯ Nivel de riesgo: {result.get('risk_level', 'N/A')}")
            print(f"ğŸ“Š Relevancia geopolÃ­tica: {result.get('geopolitical_relevance', 'N/A')}")
        else:
            print("âŒ No se pudo completar el anÃ¡lisis")
            
        return bool(result)
        
    except Exception as e:
        print(f"âŒ Error en anÃ¡lisis: {e}")
        return False

def test_summary_generation():
    """Probar generaciÃ³n de resÃºmenes"""
    print("\nğŸ§ª Probando generaciÃ³n de resÃºmenes...")
    
    try:
        from src.ai.ollama_service import ollama_service, OllamaModel
        
        title = "Crisis econÃ³mica global"
        content = """
        Los mercados financieros mundiales han experimentado una volatilidad significativa.
        Los bancos centrales estÃ¡n considerando medidas de emergencia.
        Los indicadores econÃ³micos sugieren una desaceleraciÃ³n generalizada.
        Las empresas reportan dificultades en las cadenas de suministro.
        """
        
        print("ğŸ”„ Generando resumen con Gemma...")
        
        # Resumen con Gemma (rÃ¡pido)
        summary = ollama_service.generate_lightweight_summary(
            title=title,
            content=content,
            model=OllamaModel.GEMMA2_2B
        )
        
        if summary and "error" not in summary.lower():
            print("âœ… Resumen generado")
            print(f"ğŸ“„ Resumen: {summary}")
        else:
            print(f"âŒ Error generando resumen: {summary}")
            
        return bool(summary and "error" not in summary.lower())
        
    except Exception as e:
        print(f"âŒ Error generando resumen: {e}")
        return False

def main():
    """FunciÃ³n principal"""
    print("ğŸš€ Test RÃ¡pido del Sistema de IA Unificado")
    print("=" * 50)
    
    # 1. Probar Ollama
    ollama_ok = test_ollama_integration()
    
    if not ollama_ok:
        print("\nâŒ Ollama no disponible. AsegÃºrate de que estÃ© corriendo:")
        print("   - ollama serve")
        print("   - python install_ollama.py")
        return
    
    # 2. Probar servicio unificado
    unified_ok = test_unified_service()
    
    # 3. Probar funcionalidades
    if unified_ok:
        analysis_ok = test_simple_analysis()
        summary_ok = test_summary_generation()
        
        print(f"\n{'='*50}")
        print("ğŸ“Š Resumen de Pruebas:")
        print(f"  - Ollama: {'âœ…' if ollama_ok else 'âŒ'}")
        print(f"  - Servicio unificado: {'âœ…' if unified_ok else 'âŒ'}")
        print(f"  - AnÃ¡lisis geopolÃ­tico: {'âœ…' if analysis_ok else 'âŒ'}")
        print(f"  - GeneraciÃ³n de resÃºmenes: {'âœ…' if summary_ok else 'âŒ'}")
        
        if all([ollama_ok, unified_ok, analysis_ok, summary_ok]):
            print("\nğŸ‰ Â¡Sistema completamente funcional!")
            print("ğŸ’¡ El sistema puede manejar automÃ¡ticamente los rate limits de Groq")
            print("ğŸ¤– Usando DeepSeek, Gemma, Qwen y Llama segÃºn la tarea")
        else:
            print("\nâš ï¸ Algunas funcionalidades no estÃ¡n disponibles")
    
    print(f"\nâœ… Test completado")

if __name__ == "__main__":
    main()
