# ðŸ§  IntegraciÃ³n de Ollama con DeepSeek-R1 y Gemma en RiskMap

## ðŸ“‹ Resumen

RiskMap ahora incluye integraciÃ³n completa con **Ollama** para ejecutar modelos de IA localmente, incluyendo:

- ðŸ§  **DeepSeek-R1**: Para razonamiento profundo y anÃ¡lisis geopolÃ­tico avanzado
- âš¡ **Gemma 2**: Para procesamiento rÃ¡pido y resÃºmenes eficientes  
- ðŸŒ **Qwen**: Para anÃ¡lisis multiidioma y tareas generales
- ðŸ“ **Llama 3.1**: Para generaciÃ³n de texto y anÃ¡lisis estÃ¡ndar

## ðŸš€ InstalaciÃ³n RÃ¡pida

### 1. Instalar Ollama

**Windows:**
```bash
# Descargar desde: https://ollama.com/download/windows
# O usar el script automatizado:
python install_ollama.py
```

**Verificar instalaciÃ³n:**
```bash
ollama version
```

### 2. Instalar Modelos Recomendados

```bash
# Modelos principales (ejecutar en este orden)
ollama pull deepseek-r1:7b      # AnÃ¡lisis profundo (4.1GB)
ollama pull gemma2:2b           # Procesamiento rÃ¡pido (1.6GB)
ollama pull qwen:7b             # AnÃ¡lisis general (4.1GB)

# Modelos adicionales (opcionales)
ollama pull llama3.1:8b         # GeneraciÃ³n de texto (4.7GB)
ollama pull gemma2:9b           # AnÃ¡lisis avanzado (5.4GB)
ollama pull qwen2.5-coder:7b    # Tareas tÃ©cnicas (4.1GB)
```

### 3. Configurar Variables de Entorno

```bash
# Agregar a .env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=300
AI_PROVIDER_PRIORITY=ollama,groq
USE_LOCAL_AI=true
```

## ðŸ”§ Uso del Sistema

### API Endpoints

#### Estado del Sistema
```bash
GET /api/ollama/status
```

Respuesta:
```json
{
  "success": true,
  "ollama_status": {
    "ollama": {
      "available": true,
      "models": ["deepseek-r1:7b", "gemma2:2b", "qwen:7b"],
      "base_url": "http://localhost:11434"
    },
    "capabilities": {
      "deep_reasoning": true,
      "fast_processing": true,
      "multilingual": true
    }
  }
}
```

#### AnÃ¡lisis Profundo con DeepSeek-R1
```bash
POST /api/ai/deep-analysis
Content-Type: application/json

{
  "content": "Texto a analizar...",
  "question": "Â¿CuÃ¡les son las implicaciones geopolÃ­ticas?"
}
```

#### Resumen RÃ¡pido con Gemma
```bash
POST /api/ai/fast-summary
Content-Type: application/json

{
  "title": "TÃ­tulo del artÃ­culo",
  "content": "Contenido del artÃ­culo...",
  "max_words": 100
}
```

#### AnÃ¡lisis Unificado (SelecciÃ³n AutomÃ¡tica)
```bash
POST /api/ai/unified-analysis
Content-Type: application/json

{
  "content": "Contenido a analizar...",
  "type": "deep",  // "standard", "deep", "fast"
  "prefer_local": true
}
```

### Uso ProgramÃ¡tico

```python
from src.ai.unified_ai_service import unified_ai_service
import asyncio

# AnÃ¡lisis geopolÃ­tico estÃ¡ndar
async def analyze_content():
    response = await unified_ai_service.analyze_geopolitical_content(
        content="Texto de noticia geopolÃ­tica...",
        prefer_local=True
    )
    
    if response.success:
        print(f"AnÃ¡lisis con {response.provider}: {response.content}")
        print(f"Metadatos: {response.metadata}")

# Resumen rÃ¡pido
def quick_summary():
    response = unified_ai_service.generate_fast_summary(
        title="TÃ­tulo de la noticia",
        content="Contenido de la noticia...",
        max_words=150
    )
    
    return response.content

# Razonamiento profundo
async def deep_reasoning():
    response = await unified_ai_service.perform_deep_analysis(
        content="Contexto geopolÃ­tico...",
        question="Â¿CuÃ¡les son las implicaciones a largo plazo?"
    )
    
    return response.metadata
```

## ðŸŽ¯ CaracterÃ­sticas de los Modelos

### DeepSeek-R1 ðŸ§ 
- **Especialidad**: Razonamiento profundo y anÃ¡lisis complejo
- **Uso**: AnÃ¡lisis geopolÃ­tico avanzado, evaluaciÃ³n de riesgos
- **Ventajas**: Pensamiento crÃ­tico, anÃ¡lisis paso a paso
- **Modelos**: `deepseek-r1:7b`, `deepseek-r1:14b`

### Gemma 2 âš¡
- **Especialidad**: Procesamiento rÃ¡pido y eficiente
- **Uso**: ResÃºmenes, clasificaciÃ³n rÃ¡pida, tareas ligeras
- **Ventajas**: Velocidad, bajo consumo de recursos
- **Modelos**: `gemma2:2b`, `gemma2:9b`

### Qwen ðŸŒ
- **Especialidad**: AnÃ¡lisis multiidioma y tareas generales
- **Uso**: AnÃ¡lisis de contenido internacional, programaciÃ³n
- **Ventajas**: Soporte multiidioma, versatilidad
- **Modelos**: `qwen:7b`, `qwen2.5-coder:7b`

### Llama 3.1 ðŸ“
- **Especialidad**: GeneraciÃ³n de texto y anÃ¡lisis estÃ¡ndar
- **Uso**: GeneraciÃ³n de contenido, anÃ¡lisis general
- **Ventajas**: Equilibrio entre calidad y velocidad
- **Modelos**: `llama3.1:8b`

## ðŸ”„ Flujo de Trabajo del Sistema

```mermaid
graph TD
    A[Contenido de Entrada] --> B{Tipo de AnÃ¡lisis}
    
    B -->|RÃ¡pido| C[Gemma 2B]
    B -->|Profundo| D[DeepSeek-R1 7B]
    B -->|EstÃ¡ndar| E[Qwen 7B]
    B -->|GeneraciÃ³n| F[Llama 3.1 8B]
    
    C --> G[Resumen RÃ¡pido]
    D --> H[AnÃ¡lisis Profundo]
    E --> I[AnÃ¡lisis General]
    F --> J[Contenido Generado]
    
    G --> K[Respuesta Unificada]
    H --> K
    I --> K
    J --> K
    
    K --> L{Fallback Necesario?}
    L -->|SÃ­| M[Groq API]
    L -->|No| N[Resultado Final]
    M --> N
```

## ðŸ§ª Pruebas del Sistema

### Ejecutar Suite de Pruebas
```bash
python test_ollama_integration.py
```

### Pruebas Manuales

1. **Verificar Ollama**:
   ```bash
   ollama list
   ```

2. **Probar modelo especÃ­fico**:
   ```bash
   ollama run deepseek-r1:7b "Explica la situaciÃ³n geopolÃ­tica actual"
   ```

3. **API de estado**:
   ```bash
   curl http://localhost:5000/api/ollama/status
   ```

## ðŸ“Š Requisitos del Sistema

### MÃ­nimos
- **RAM**: 8GB (para modelos 2B-7B)
- **Almacenamiento**: 15GB libres
- **CPU**: 4 nÃºcleos (recomendado)

### Recomendados
- **RAM**: 16GB+ (para modelos 7B-14B)
- **GPU**: NVIDIA con CUDA (opcional, acelera procesamiento)
- **Almacenamiento**: SSD para mejor rendimiento

### Espacio por Modelo
- `gemma2:2b`: ~1.6GB
- `deepseek-r1:7b`: ~4.1GB  
- `qwen:7b`: ~4.1GB
- `llama3.1:8b`: ~4.7GB
- `deepseek-r1:14b`: ~8.2GB

## ðŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno Completas
```bash
# ConfiguraciÃ³n de Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=300
OLLAMA_MAX_RETRIES=3

# ConfiguraciÃ³n del servicio unificado  
AI_PROVIDER_PRIORITY=ollama,groq
USE_LOCAL_AI=true
PREFER_LOCAL_ANALYSIS=true

# ConfiguraciÃ³n de modelos por tarea
ANALYSIS_MODEL=deepseek-r1:7b
SUMMARY_MODEL=gemma2:2b
GENERATION_MODEL=llama3.1:8b
MULTILINGUAL_MODEL=qwen:7b
```

### OptimizaciÃ³n de Rendimiento
```python
# En unified_ai_service.py
config = OllamaConfig(
    base_url="http://localhost:11434",
    timeout=600,  # Para anÃ¡lisis profundo
    temperature=0.2,  # MÃ¡s determinista
    max_tokens=4000,
    top_p=0.9,
    top_k=40
)
```

## ðŸš¨ SoluciÃ³n de Problemas

### Problemas Comunes

1. **Ollama no responde**:
   ```bash
   # Reiniciar servicio
   ollama serve
   ```

2. **Modelo no encontrado**:
   ```bash
   # Verificar modelos instalados
   ollama list
   
   # Instalar modelo faltante
   ollama pull deepseek-r1:7b
   ```

3. **Error de memoria**:
   - Usar modelos mÃ¡s pequeÃ±os (2B en lugar de 7B)
   - Cerrar aplicaciones innecesarias
   - Aumentar memoria virtual

4. **Timeout en anÃ¡lisis**:
   - Aumentar `OLLAMA_TIMEOUT` en .env
   - Usar modelos mÃ¡s rÃ¡pidos para pruebas

### Logs y Debugging
```python
import logging
logging.getLogger('src.ai').setLevel(logging.DEBUG)
```

## ðŸ”® PrÃ³ximas CaracterÃ­sticas

- [ ] IntegraciÃ³n con GPU para aceleraciÃ³n
- [ ] CachÃ© inteligente de respuestas
- [ ] Modelos especializados por regiÃ³n geogrÃ¡fica
- [ ] AnÃ¡lisis en tiempo real con streaming
- [ ] Interfaz web para gestiÃ³n de modelos
- [ ] MÃ©tricas de rendimiento por modelo

## ðŸ“š Referencias

- [Ollama Documentation](https://ollama.com/docs)
- [DeepSeek-R1 Model Card](https://huggingface.co/deepseek-ai/DeepSeek-R1)
- [Gemma 2 Technical Report](https://arxiv.org/abs/2408.00118)
- [Qwen2.5 Documentation](https://qwenlm.github.io/blog/qwen2.5/)

---

## ðŸŽ‰ Â¡Sistema Listo!

Con esta integraciÃ³n, RiskMap ahora cuenta con capacidades de IA local avanzadas:

- ðŸ”’ **Privacidad**: AnÃ¡lisis completamente local
- âš¡ **Velocidad**: Sin lÃ­mites de API externa
- ðŸ§  **Inteligencia**: Modelos especializados por tarea
- ðŸ”„ **Redundancia**: Fallback automÃ¡tico a Groq
- ðŸ“ˆ **Escalabilidad**: Agregar nuevos modelos fÃ¡cilmente

Â¡El futuro del anÃ¡lisis geopolÃ­tico inteligente estÃ¡ aquÃ­! ðŸš€
